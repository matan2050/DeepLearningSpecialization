{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Build a baseline model (5 marks) \n",
    "Use the Keras library to build a neural network with the following:\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "- Use the adam optimizer and the mean squared error as the loss function.\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_split\n",
    "helper function from Scikit-learn.\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 00:49:33.672879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-08 00:49:41.637188: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-08 00:49:41.646902: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 12ms/step\n",
      "10/10 [==============================] - 0s 14ms/step\n",
      "10/10 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "\n",
    "# set a variable to hold the list of columns \n",
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "# split the data into predictors and target\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column\n",
    "\n",
    "# save the count of columns in the predictors for later use\n",
    "n_cols = predictors.shape[1] # number of predictors\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define a function that contains our regression model\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = regression_model()\n",
    "\n",
    "# import scikit-learn package to split the data into train and test\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import scikit-learn package to report the mean squared error for each run\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize an empty list to store mean squared errors\n",
    "mse_list = []\n",
    "\n",
    "# Repeat the training and evaluation process 50 times\n",
    "for i in range(50):\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(predictors, target, test_size=.3, random_state=1)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "    # Append the mean squared error to the list\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Calculate the mean and standard deviation of the mean squared errors\n",
    "mean_mse = sum(mse_list) / len(mse_list)\n",
    "std_dev_mse = pd.Series(mse_list).std()\n",
    "\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Standard Deviation of MSE:\", std_dev_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Run A\n",
    "\n",
    "Mean MSE: 68.28\n",
    "\n",
    "Standard Deviation of MSE: 18.33\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Normalize the data (5 marks) \n",
    "Repeat Part A but use a normalized version of the data. Recall that one way to normalize the data is by subtracting the mean from the individual predictors and dividing by the standard deviation.\n",
    "How does the mean of the mean squared errors compare to that from Step A?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 209us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 160us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 46us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 277us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 754us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 543us/step\n",
      "10/10 [==============================] - 0s 302us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 626us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 280us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "Mean MSE: 61.89915689822899\n",
      "Standard Deviation of MSE: 47.72437943972294\n"
     ]
    }
   ],
   "source": [
    "# normalize the data by subtracting the mean and dividing by the standard deviation\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "\n",
    "# save the count of columns in the predictors for later use\n",
    "n_cols = predictors_norm.shape[1] # number of predictors\n",
    "\n",
    "# Initialize an empty list to store mean squared errors\n",
    "mse_list = []\n",
    "\n",
    "# Repeat the training and evaluation process 50 times\n",
    "for i in range(50):\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(predictors_norm, target, test_size=.3, random_state=1)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "    # Append the mean squared error to the list\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Calculate the mean and standard deviation of the mean squared errors\n",
    "mean_mse = sum(mse_list) / len(mse_list)\n",
    "std_dev_mse = pd.Series(mse_list).std()\n",
    "\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Standard Deviation of MSE:\", std_dev_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Run B\n",
    "\n",
    "Mean MSE: 61.90, roughly 20% lower than reported after run A\n",
    "\n",
    "Standard Deviation of MSE: 47.72, roughly 150% higher than reported after run A, representing a flattened-out normal curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Increate the number of epochs (5 marks)\n",
    "Repeat Part B but use 100 epochs this time for training.\n",
    "How does the mean of the mean squared errors compare to that from Step B?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 794us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 786us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 208us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 729us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 269us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 113us/step\n",
      "10/10 [==============================] - 0s 129us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 488us/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 274us/step\n",
      "10/10 [==============================] - 0s 167us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 163us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 397us/step\n",
      "10/10 [==============================] - 0s 211us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 896us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "Mean MSE: 43.40260840213568\n",
      "Standard Deviation of MSE: 0.5430660647234798\n"
     ]
    }
   ],
   "source": [
    "# normalize the data by subtracting the mean and dividing by the standard deviation\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "\n",
    "# save the count of columns in the predictors for later use\n",
    "n_cols = predictors_norm.shape[1] # number of predictors\n",
    "\n",
    "# Initialize an empty list to store mean squared errors\n",
    "mse_list = []\n",
    "\n",
    "# Repeat the training and evaluation process 100 times\n",
    "for i in range(100):\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(predictors_norm, target, test_size=.3, random_state=1)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "    # Append the mean squared error to the list\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Calculate the mean and standard deviation of the mean squared errors\n",
    "mean_mse = sum(mse_list) / len(mse_list)\n",
    "std_dev_mse = pd.Series(mse_list).std()\n",
    "\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Standard Deviation of MSE:\", std_dev_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Run C\n",
    "\n",
    "Mean MSE: 43.40, roughly 30% lower than reported in run B due to doubling the number of iterations\n",
    "\n",
    "Standard Deviation of MSE: 0.54, a very very small standard deviation compared to run B, meaning that the mean standard errors exhibit very low variation from the mean above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Increase the number of hidden layers (5 marks)\n",
    "Repeat part B but use a neural network with the following instead:\n",
    "- Three hidden layers, each of 10 nodes and ReLU activation function.\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 0s/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 366us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 223us/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "Mean MSE: 46.25749077512327\n",
      "Standard Deviation of MSE: 23.913135427891458\n"
     ]
    }
   ],
   "source": [
    "# normalize the data by subtracting the mean and dividing by the standard deviation\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "\n",
    "# save the count of columns in the predictors for later use\n",
    "n_cols = predictors_norm.shape[1] # number of predictors\n",
    "\n",
    "# define a function that contains our regression model\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = regression_model()\n",
    "\n",
    "# Initialize an empty list to store mean squared errors\n",
    "mse_list = []\n",
    "\n",
    "# Repeat the training and evaluation process 50 times\n",
    "for i in range(50):\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(predictors_norm, target, test_size=.3, random_state=1)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "    # Append the mean squared error to the list\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Calculate the mean and standard deviation of the mean squared errors\n",
    "mean_mse = sum(mse_list) / len(mse_list)\n",
    "std_dev_mse = pd.Series(mse_list).std()\n",
    "\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Standard Deviation of MSE:\", std_dev_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Run D\n",
    "\n",
    "Mean MSE: 46.26, a bit smaller than run B due to adding two more hidden layers and improving the reliability of the model\n",
    "\n",
    "Standard Deviation of MSE: 23.91, half of the standard deviation reported in run B, also indicating increased reliability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
